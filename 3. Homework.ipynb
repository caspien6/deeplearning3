{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib import request, parse\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collected the data from http://idojarasbudapest.hu/archivalt-idojaras with a lot of HTTP POST method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPrepare:\n",
    "    '''\n",
    "    This class will download and clean the data, what comes from the above url\n",
    "    '''\n",
    "    def __init__(self, url, interval_start, interval_end):\n",
    "        ''' Url - the endpoint where to post\n",
    "            interval_start - datetime.date type!!\n",
    "            interval_end - datetime.date type!!\n",
    "        '''\n",
    "        np.random.seed(1)\n",
    "        self.url = url\n",
    "        self.dstart = interval_start\n",
    "        self.dend = interval_end\n",
    "    \n",
    "    def htmltable_to_df(self,html):\n",
    "        '''Find the table html tag from the response,\n",
    "            create table columns and add data to it.\n",
    "            html - html file with one <table> tag.\n",
    "        '''\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        table = soup.find(\"table\")\n",
    "        # The first tr contains the field names.\n",
    "        headings = [th.get_text().strip() for th in table.find(\"tr\").find_all(\"th\")]\n",
    "        datasets = []\n",
    "        for row in table.find_all(\"tr\")[1:]:\n",
    "            dataset = dict(zip(headings, (td.get_text() for td in row.find_all(\"td\"))))\n",
    "            datasets.append(dataset)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(datasets)\n",
    "        return df\n",
    "    \n",
    "    def post_query(self,mydate):\n",
    "        '''Posts a query request to the above define url with the added date.'''\n",
    "        self.data = parse.urlencode({'ev': mydate.year, 'ho': str(mydate.month).zfill(2), 'button': 'Mehet'}).encode()\n",
    "        req =  request.Request(self.url, data=self.data) # this will make the method \"POST\"\n",
    "        resp = request.urlopen(req)\n",
    "        html = resp.read().decode()\n",
    "        return html\n",
    "    \n",
    "    def collect_data(self):\n",
    "        ''' Iterate over start to end date monthly,\n",
    "            download monthly data, append to the existing dataframe.\n",
    "        '''\n",
    "        month = relativedelta(months=+1)\n",
    "        mydate = self.dstart\n",
    "        while mydate <= self.dend:\n",
    "            html = self.post_query(mydate)\n",
    "            df_tmp = self.htmltable_to_df(html)\n",
    "            \n",
    "            if mydate == self.dstart:\n",
    "                self.dataframe = df_tmp\n",
    "            else:\n",
    "                self.dataframe = self.dataframe.append(df_tmp, ignore_index=True)\n",
    "            \n",
    "            mydate += month\n",
    "        return self.dataframe\n",
    "    \n",
    "    def clean_data(self):\n",
    "        ''' Clean the column names, drop columns what i don't needed, rename and create \n",
    "        '''\n",
    "        regex = re.compile(\"([a-z]+)\")\n",
    "        self.dataframe['Nap'] = self.dataframe['Nap'].apply(lambda x: regex.split(x)[0])\n",
    "        self.dataframe = self.dataframe.rename({self.dataframe.columns[2]: 'Tmean',\n",
    "                                   self.dataframe.columns[1]: 'Date',\n",
    "                                   self.dataframe.columns[3]: 'Tmax',\n",
    "                                   self.dataframe.columns[4]: 'Tmin'}, axis='columns')\n",
    "        \n",
    "        self.dataframe['Tmax'] = self.dataframe['Tmax'].astype(float)\n",
    "        self.dataframe['Tmin'] = self.dataframe['Tmin'].astype(float)\n",
    "        self.dataframe['Tmean'] = self.dataframe['Tmax'] + self.dataframe['Tmin'] / 2\n",
    "        self.dataframe = self.dataframe.drop([self.dataframe.columns[0], 'Tmax', 'Tmin', 'Date'], axis=1)\n",
    "        return self.dataframe\n",
    "    \n",
    "    def save_data(self, filename = 'weather_datas.csv'):\n",
    "        self.dataframe.to_csv(filename, index = False)\n",
    "    def load_data(self, filename = 'weather_datas.csv'):\n",
    "        self.dataframe = pd.read_csv(filename)        \n",
    "    \n",
    "    # convert series to supervised learning\n",
    "    def series_to_deeplr_dataset(self,data, n_in=1, n_out=1, dropnan=True):\n",
    "        '''\n",
    "            This method transform the data with this form\n",
    "            | date | temperature |\n",
    "            | 2018 | 1.2         |\n",
    "            \n",
    "            to this\n",
    "             \n",
    "            | index | var(t-1) | var(t) | var(t+1) | ... |\n",
    "            | 1     | 11       | 12     | 13       | ... |\n",
    "            | 2     | 12       | 13     | 14       | ... |\n",
    "            \n",
    "            where n_in is the number t-n_in, t-(n_in-1)...\n",
    "            where n_out is the number t+n_in, t+(n_in-1)...       \n",
    "        '''\n",
    "        n_vars = 1\n",
    "        df = pd.DataFrame(data)\n",
    "        cols, names = list(), list()\n",
    "        # input sequence (t-n, ... t-1)\n",
    "        for i in range(n_in, 0, -1):\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # forecast sequence (t, t+1, ... t+n)\n",
    "        for i in range(0, n_out):\n",
    "            cols.append(df.shift(-i))\n",
    "            if i == 0:\n",
    "                names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "            else:\n",
    "                names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "        # put it all together\n",
    "        agg = pd.concat(cols, axis=1)\n",
    "        agg.columns = names\n",
    "        # drop rows with NaN values\n",
    "        if dropnan:\n",
    "            agg.dropna(inplace=True)\n",
    "        return agg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startDate = date(2012,1,1)\n",
    "endDate = date(2018,10,1)\n",
    "url = 'http://idojarasbudapest.hu/archivalt-idojaras'\n",
    "dp = DataPrepare(url, startDate, endDate)\n",
    "\n",
    "'''dp.collect_data()\n",
    "dp.clean_data()\n",
    "dp.save_data()'''\n",
    "dp.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dp.series_to_deeplr_dataset(dp.dataframe, 2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras import regularizers\n",
    "\n",
    "class TrainingClasses:\n",
    "    '''\n",
    "        Aggregated class for the models.\n",
    "        There are three neural network what are predicting forward 1, 7, 31 day mean temperature.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        ;\n",
    "        \n",
    "    def train_daily_predictor(self, dp, day_count_before = 7, day_count_after = 1, test_split = 1/6, valid_split = 1/6,\n",
    "                             ep = 1000, batch_siz = 16):\n",
    "        dataset = dp.series_to_deeplr_dataset(dp.dataframe, n_in=day_count_before, n_out=day_count_after).values\n",
    "        \n",
    "        X = dataset[:,0:day_count_before]\n",
    "        Y = dataset[:,day_count_before:]\n",
    "\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        v_index = int(X.shape[0]*(1-valid_split-test_split))\n",
    "        t_index = int(X.shape[0]*(1-test_split))\n",
    "        \n",
    "        self.X_daily_test = X[t_index:]\n",
    "        self.Y_daily_test = Y[t_index:]\n",
    "        X_valid = X[v_index:t_index]\n",
    "        Y_valid = Y[v_index:t_index]\n",
    "        X = X[:v_index]\n",
    "        Y = Y[:v_index]\n",
    "        \n",
    "        self.daily_test_datestart = date(2012,1,1) + relativedelta(days=+t_index+day_count_before)\n",
    "        self.daily_test_dateends = self.daily_test_datestart + relativedelta(days=+self.X_daily_test.shape[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        patience=40\n",
    "        early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "        checkpointer=ModelCheckpoint(filepath='weights_daily.hdf5', save_best_only=True, verbose=1)\n",
    "        self.daily_model = self.create_daily_predictor(day_count_before, day_count_after)\n",
    "        \n",
    "        print(self.daily_model.summary())\n",
    "        # fit model\n",
    "        history = self.daily_model.fit(X, Y, epochs=ep, verbose=2, batch_size=batch_siz, validation_data=(X_valid, Y_valid),\n",
    "                 callbacks=[checkpointer, early_stopping])\n",
    "        return history\n",
    "        \n",
    "        \n",
    "    def create_daily_predictor(self, day_count_before,day_count_after):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=32, kernel_size=day_count_before-1,strides=4,padding='same', activation='relu', input_shape=(day_count_before, day_count_after)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=4, kernel_size=3, padding='same',strides=2, activation='relu' ))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "        model.add(Dense(day_count_after))\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "        self.daily_model = model\n",
    "        return model\n",
    "    \n",
    "    def train_weekly_predictor(self, dp, day_count_before = 31, day_count_after = 7, test_split = 1/6, valid_split = 1/6,\n",
    "                             ep = 1000, batch_siz = 512):\n",
    "        dataset = dp.series_to_deeplr_dataset(dp.dataframe, n_in=day_count_before, n_out=day_count_after).values\n",
    "        \n",
    "        X = dataset[:,0:day_count_before]\n",
    "        Y = dataset[:,day_count_before:]\n",
    "\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        v_index = int(X.shape[0]*(1-valid_split-test_split))\n",
    "        t_index = int(X.shape[0]*(1-test_split))\n",
    "\n",
    "        self.X_weekly_test = X[t_index:]\n",
    "        self.Y_weekly_test = Y[t_index:]\n",
    "        X_valid = X[v_index:t_index]\n",
    "        Y_valid = Y[v_index:t_index]\n",
    "        X = X[:v_index]\n",
    "        Y = Y[:v_index]\n",
    "        \n",
    "        self.weekly_test_datestart = date(2012,1,1) + relativedelta(days=+t_index+day_count_before)\n",
    "        self.weekly_test_dateends = self.weekly_test_datestart + relativedelta(days=+self.X_weekly_test.shape[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        patience=40\n",
    "        early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "        checkpointer=ModelCheckpoint(filepath='weights_weekly.hdf5', save_best_only=True, verbose=1)\n",
    "        self.weekly_model = self.create_weekly_predictor(day_count_before, day_count_after)\n",
    "        \n",
    "        print(self.weekly_model.summary())\n",
    "        # fit model\n",
    "        histoy = self.weekly_model.fit(X, Y, epochs=ep, verbose=2, batch_size=batch_siz, validation_data=(X_valid, Y_valid),\n",
    "                 callbacks=[checkpointer, early_stopping])\n",
    "        return history\n",
    "        \n",
    "        \n",
    "    def create_weekly_predictor(self, day_count_before,day_count_after):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=32, kernel_size=31, strides=21, padding = 'same', activation='relu', input_shape=(day_count_before, 1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=8, kernel_size=7, strides=3, padding = 'same', activation='relu', input_shape=(day_count_before, 1)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "        model.add(Dense(day_count_after))\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "        self.weekly_model = model\n",
    "        return self.weekly_model\n",
    "    \n",
    "    def train_monthly_predictor(self, dp, day_count_before = 31, day_count_after = 31, test_split = 1/6, valid_split = 1/6,\n",
    "                             ep = 1000, batch_siz = 512):\n",
    "        dataset = dp.series_to_deeplr_dataset(dp.dataframe, n_in=day_count_before, n_out=day_count_after).values\n",
    "        \n",
    "        X = dataset[:,0:day_count_before]\n",
    "        Y = dataset[:,day_count_before:]\n",
    "        print(date(2012,1,1) + relativedelta(days=+X.shape[0]))\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        v_index = int(X.shape[0]*(1-valid_split-test_split))\n",
    "        t_index = int(X.shape[0]*(1-test_split))\n",
    "\n",
    "        self.X_monthly_test = X[t_index:]\n",
    "        self.Y_monthly_test = Y[t_index:]\n",
    "        X_valid = X[v_index:t_index]\n",
    "        Y_valid = Y[v_index:t_index]\n",
    "        X = X[:v_index]\n",
    "        Y = Y[:v_index]\n",
    "        \n",
    "        self.monthly_test_datestart = date(2012,1,1) + relativedelta(days=+t_index+day_count_before)\n",
    "        self.monthly_test_dateends = self.monthly_test_datestart + relativedelta(days=+self.X_monthly_test.shape[0])\n",
    "        \n",
    "        patience=40\n",
    "        early_stopping=EarlyStopping(patience=patience, verbose=1)\n",
    "        checkpointer=ModelCheckpoint(filepath='weights_weekly.hdf5', save_best_only=True, verbose=1)\n",
    "        self.monthly_model = self.create_monthly_predictor(day_count_before, day_count_after)\n",
    "        \n",
    "        print(self.monthly_model.summary())\n",
    "        # fit model\n",
    "        histoy = self.monthly_model.fit(X, Y, epochs=ep, verbose=2, batch_size=batch_siz, validation_data=(X_valid, Y_valid),\n",
    "                 callbacks=[checkpointer, early_stopping])\n",
    "        \n",
    "        return history\n",
    "        \n",
    "        \n",
    "    def create_monthly_predictor(self, day_count_before,day_count_after):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=31,padding='same', activation='relu', input_shape=(day_count_before, 1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=16, kernel_size=7, activation='relu', input_shape=(day_count_before, 1)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "        model.add(Dense(day_count_after))\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "        self.monthly_model = model\n",
    "        return self.monthly_model\n",
    "        \n",
    "    def test_daily_predictor(self, prediction_count = 10):\n",
    "        \n",
    "\n",
    "        results = []\n",
    "        for i in range(self.X_daily_test.shape[0]):\n",
    "            x_input = self.X_daily_test[i]\n",
    "            x_input = x_input.reshape((1, self.X_daily_test.shape[1], 1))\n",
    "            yhat = self.daily_model.predict(x_input, verbose=0)\n",
    "            results.append(yhat.flatten())\n",
    "\n",
    "        date_time = [ self.daily_test_datestart + relativedelta(days=d) for d in range(0, len(results))]\n",
    "        lastdate = date_time[-1]\n",
    "        \n",
    "        date_time = pd.to_datetime(date_time)\n",
    "\n",
    "        DF = pd.DataFrame()\n",
    "        DF['temp'] = results\n",
    "        DF = DF.set_index(date_time)\n",
    "        \n",
    "        res = []\n",
    "            \n",
    "        # prediction\n",
    "        for i in range(prediction_count):\n",
    "            x_input = np.array(results[-self.X_daily_test.shape[1]:])\n",
    "            x_input = x_input.reshape((1, self.X_daily_test.shape[1], 1))\n",
    "            yhat = self.daily_model.predict(x_input, verbose=0)\n",
    "            res.extend(yhat.flatten().tolist())\n",
    "    \n",
    "        date_time2 = [ lastdate + relativedelta(days=d) for d in range(1, len(res)+1)]\n",
    "        date_time2 = pd.to_datetime(date_time2)\n",
    "        DF_tmp = pd.DataFrame()\n",
    "        DF_tmp['temp'] = res\n",
    "        DF_tmp = DF_tmp.set_index(date_time2)\n",
    "        DF = DF.append(DF_tmp)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        fig.subplots_adjust(bottom=0.3)\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "        ax.axvline(x=pd.to_datetime(lastdate), color='g', linestyle='--')\n",
    "        \n",
    "        xposition = [pd.to_datetime('2018-10-26'), pd.to_datetime('2018-11-01'),pd.to_datetime('2018-11-29')]\n",
    "        print(xposition[0], ' mean temperature prediction: ', DF.loc[xposition[0]][0], ' °C')\n",
    "        print(xposition[1], ' mean temperature prediction: ', DF.loc[xposition[1]][0], ' °C')\n",
    "        print(xposition[2], ' mean temperature prediction: ', DF.loc[xposition[2]][0], ' °C')\n",
    "        \n",
    "        for xc in xposition:\n",
    "            ax.axvline(x=xc, color='r', linestyle='--')\n",
    "        fig.suptitle('Daily predictor, predcition starts from the green line.', fontsize=12, fontweight='bold')\n",
    "        plt.plot(DF)\n",
    "    \n",
    "    def test_weekly_predictor(self, prediction_count = 3):\n",
    "\n",
    "        results = []\n",
    "        for i in range(0,self.X_weekly_test.shape[0],self.Y_weekly_test.shape[1]):\n",
    "            x_input = self.X_weekly_test[i]\n",
    "            x_input = x_input.reshape((1, self.X_weekly_test.shape[1], 1))\n",
    "            \n",
    "            yhat = self.weekly_model.predict(x_input, verbose=0)\n",
    "\n",
    "            results.extend(yhat.flatten().tolist())\n",
    "                \n",
    "        \n",
    "        date_time = [ self.weekly_test_datestart + relativedelta(days=d) for d in range(0, len(results))]\n",
    "        lastdate = date_time[-1]\n",
    "        \n",
    "        date_time = pd.to_datetime(date_time)\n",
    "\n",
    "        DF = pd.DataFrame()\n",
    "        DF['temp'] = results\n",
    "        DF = DF.set_index(date_time)\n",
    "        \n",
    "        res = []\n",
    "        res.extend(results[-self.X_weekly_test.shape[1]:])\n",
    "        for i in range(prediction_count):\n",
    "            x_input = np.array(res[-self.X_weekly_test.shape[1]:])\n",
    "            x_input = x_input.reshape((1, self.X_weekly_test.shape[1], 1))\n",
    "            yhat = self.weekly_model.predict(x_input, verbose=0)\n",
    "            res.extend(yhat.flatten().tolist())\n",
    "            \n",
    "        date_time2 = [ lastdate + relativedelta(days=d) for d in range(1, len(res)+1)]\n",
    "        date_time2 = pd.to_datetime(date_time2)\n",
    "        DF_tmp = pd.DataFrame()\n",
    "        DF_tmp['temp'] = res\n",
    "        DF_tmp = DF_tmp.set_index(date_time2)\n",
    "        DF = DF.append(DF_tmp)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        fig.subplots_adjust(bottom=0.3)\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "        ax.axvline(x=pd.to_datetime(lastdate), color='g', linestyle='--')\n",
    "        \n",
    "        xposition = [pd.to_datetime('2018-10-26'), pd.to_datetime('2018-11-01'),pd.to_datetime('2018-11-29')]\n",
    "        print(xposition[0], ' mean temperature prediction: ', DF.loc[xposition[0]][0], ' °C')\n",
    "        print(xposition[1], ' mean temperature prediction: ', DF.loc[xposition[1]][0], ' °C')\n",
    "        print(xposition[2], ' mean temperature prediction: ', DF.loc[xposition[2]][0], ' °C')\n",
    "        \n",
    "        for xc in xposition:\n",
    "            ax.axvline(x=xc, color='r', linestyle='--')\n",
    "        fig.suptitle('Weekly predictor, predcition starts from the green line.', fontsize=8, fontweight='bold')\n",
    "        plt.plot(DF)\n",
    "        \n",
    "    def test_monthly_predictor(self, prediction_count = 3):\n",
    "\n",
    "        results = []\n",
    "        '''for i in range(0,self.X_monthly_test.shape[0],self.Y_monthly_test.shape[1]):\n",
    "            x_input = self.X_monthly_test[i]\n",
    "            x_input = x_input.reshape((1, self.X_monthly_test.shape[1], 1))\n",
    "            \n",
    "            yhat = self.monthly_model.predict(x_input, verbose=0)\n",
    "\n",
    "            results.extend(yhat.flatten().tolist())\n",
    "        '''\n",
    "        \n",
    "        for i in range(0,self.Y_monthly_test.shape[0],self.Y_monthly_test.shape[1]):\n",
    "            y_out = self.Y_monthly_test[i]\n",
    "\n",
    "            results.extend(y_out.flatten().tolist())\n",
    "        \n",
    "        \n",
    "        \n",
    "        date_time = [ self.monthly_test_datestart + relativedelta(days=d) for d in range(0, len(results))]\n",
    "        lastdate = date_time[-1]\n",
    "        print(lastdate)\n",
    "        date_time = pd.to_datetime(date_time)\n",
    "\n",
    "        DF = pd.DataFrame()\n",
    "        DF['temp'] = results\n",
    "        DF = DF.set_index(date_time)\n",
    "        \n",
    "        res = []\n",
    "        res.extend(results[-self.X_monthly_test.shape[1]:])\n",
    "        for i in range(prediction_count):\n",
    "            x_input = np.array(res[-(self.X_monthly_test.shape[1]):])\n",
    "            x_input = x_input.reshape((1, self.X_monthly_test.shape[1], 1))\n",
    "            yhat = self.monthly_model.predict(x_input, verbose=0)\n",
    "            res.extend(yhat.flatten().tolist())\n",
    "            \n",
    "        date_time2 = [ lastdate + relativedelta(days=d) for d in range(1, len(res)+1)]\n",
    "        date_time2 = pd.to_datetime(date_time2)\n",
    "        DF_tmp = pd.DataFrame()\n",
    "        DF_tmp['temp'] = res\n",
    "        DF_tmp = DF_tmp.set_index(date_time2)\n",
    "        DF = DF.append(DF_tmp)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        fig.subplots_adjust(bottom=0.3)\n",
    "        plt.xticks(rotation=90)\n",
    "        \n",
    "        ax.axvline(x=pd.to_datetime(lastdate), color='g', linestyle='--')\n",
    "        \n",
    "        xposition = [pd.to_datetime('2018-10-26'), pd.to_datetime('2018-11-01'),pd.to_datetime('2018-11-29')]\n",
    "        print(xposition[0], ' mean temperature prediction: ', DF.loc[xposition[0]][0], ' °C')\n",
    "        print(xposition[1], ' mean temperature prediction: ', DF.loc[xposition[1]][0], ' °C')\n",
    "        print(xposition[2], ' mean temperature prediction: ', DF.loc[xposition[2]][0], ' °C')\n",
    "        \n",
    "        for xc in xposition:\n",
    "            ax.axvline(x=xc, color='r', linestyle='--')\n",
    "        fig.suptitle('Monthly predictor, predcition from the predicted elements starts from the green line.', fontsize=8, fontweight='bold')\n",
    "        plt.plot(DF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history_losses(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#dataset = dp.series_to_deeplr_dataset(dp.dataframe, n_in=day_count_before, n_out=day_count_after).values\n",
    "model_holder = TrainingClasses()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_holder.train_weekly_predictor(dp,day_count_before = 62, day_count_after = 7, test_split = 1/5, valid_split = 0.1,\n",
    "                             ep = 100, batch_siz = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_holder.test_weekly_predictor(4)\n",
    "print('Prediction starts from: ', model_holder.weekly_test_dateends)\n",
    "plot_history_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_holder.train_monthly_predictor(dp,day_count_before = 93, day_count_after = 31, test_split = 1/6, valid_split = 1/6,\n",
    "                             ep = 1000, batch_siz = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_holder.test_monthly_predictor(3) # 3 prediction\n",
    "print('Prediction starts from: ', model_holder.monthly_test_dateends)\n",
    "plot_history_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_holder.train_daily_predictor(dp,day_count_before = 7, day_count_after = 1, test_split = 1/6, valid_split = 1/6,\n",
    "                             ep = 1000, batch_siz = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_holder.test_daily_predictor(50)\n",
    "print('Prediction starts from: ', model_holder.daily_test_dateends)\n",
    "plot_history_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
